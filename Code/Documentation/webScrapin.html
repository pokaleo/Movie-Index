<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>Backend.webScrapin API documentation</title>
<meta name="description" content="Author: Yvonne Ding
Date: 7th March 2023
Description: Web-scraping IMDb info and convert to XML format
methods
Usage example:
python3 webScrap.py â€¦" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>Backend.webScrapin</code></h1>
</header>
<section id="section-intro">
<hr>
<p>Author: Yvonne Ding
Date: 7th March 2023
Description: Web-scraping IMDb info and convert to XML format
methods
Usage example:
python3 webScrap.py &ndash;url <a href="https://www.imdb.com/search/title/?release_date=2010-01-01,2023-01-01">https://www.imdb.com/search/title/?release_date=2010-01-01,2023-01-01</a> &ndash;url_baseline
<a href="https://www.imdb.com">https://www.imdb.com</a></p>
<hr>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;
------------------------------------------------------------
Author: Yvonne Ding
Date: 7th March 2023
Description: Web-scraping IMDb info and convert to XML format
methods
Usage example:
python3 webScrap.py --url https://www.imdb.com/search/title/?release_date=2010-01-01,2023-01-01 --url_baseline
https://www.imdb.com
------------------------------------------------------------
&#34;&#34;&#34;

import argparse
import requests
from bs4 import BeautifulSoup
from collections import defaultdict
import re
import os
import xml.etree.ElementTree as ET
import xml.dom.minidom


class webScraping:
    def __init__(self, url, url_baseline):
        self.url = url
        self.url_baseline = url_baseline
        self.doc_id = []  # link to access the page of video details
        self.allInfo = defaultdict(lambda: defaultdict(str))  # {id1: {title: , year: , type: ,colorinfo: ,}, id2:{...}}

    def certainResponse(self, container, curr_class, curr_str):
        res = []
        if curr_str == &#39;runtime&#39;:
            temp = container.p.find(curr_class, class_=curr_str)
            if temp == None:
                return None
            else:
                res.append(container.p.find(curr_class, class_=curr_str).text.replace(&#39; min&#39;, &#39;&#39;))
                return res
        elif curr_str == &#39;certificate&#39;:
            data_cer = container.find(curr_class, attrs={&#39;class&#39;: &#39;ipl-zebra-list__item&#39;, &#39;id&#39;: &#39;certifications-list&#39;})
            cert_ = data_cer.find_all(&#39;li&#39;, attrs={&#39;class&#39;: &#39;ipl-inline-list__item&#39;})
            for i in cert_:
                raw_ = i.a.text.split(&#39;:&#39;)
                res.append((raw_[0], raw_[1]))
            return res
        elif curr_str == &#39;genre&#39;:
            res = []
            temp = container.p.find(curr_class, class_=curr_str)
            if temp == None:
                return None
            else:
                raw_ = container.find(curr_class, class_=curr_str).text.replace(&#39;\n&#39;, &#39;&#39;)
                return re.sub(&#39; +&#39;, &#39;&#39;, raw_).split(&#39;,&#39;)
        elif curr_str == &#39;color&#39;:
            res = []
            temp = container.find(curr_class,
                                  attrs={&#39;class&#39;: &#39;ipc-metadata-list__item&#39;, &#39;data-testid&#39;: &#39;title-techspec_color&#39;})
            if temp == None:
                return None
            else:
                res.append(temp.a.text)
                return res
        elif curr_str == &#39;soundmixes&#39;:
            temp = container.find(curr_class,
                                  attrs={&#39;class&#39;: &#39;ipc-metadata-list__item&#39;, &#39;data-testid&#39;: &#39;title-techspec_soundmix&#39;})
            if temp == None:
                return None
            else:
                res.append(temp.a.text)
                return res
        elif curr_str == &#39;keywords&#39;:
            k_list = []
            data_keyword_ = container.findAll(curr_class, attrs={&#39;class&#39;: &#39;soda sodavote&#39;})
            for i in data_keyword_:
                k_list.append(i[&#39;data-item-keyword&#39;])
            return k_list
        elif curr_str == &#39;country&#39;:
            data_country = container.find(curr_class, attrs={&#39;class&#39;: &#39;ipc-metadata-list__item&#39;,
                                                             &#39;data-testid&#39;: &#39;title-details-origin&#39;})
            res.append(data_country.a.text)
            return res
        elif curr_str == &#39;releasedate&#39;:
            date_list = []
            data_released = container.find(curr_class, attrs={&#39;data-testid&#39;: &#39;sub-section-releases&#39;,
                                                              &#39;class&#39;: &#39;sc-f65f65be-0 fVkLRr&#39;})
            date_ = data_released.find_all(&#39;li&#39;, attrs={&#39;data-testid&#39;: &#39;list-item&#39;})
            for i in date_:
                date_ = &#39;&#39;.join(i.label.text.split(&#39;,&#39;))
                date_list.append((i.a[&#39;aria-label&#39;], date_))
            return date_list
            # res.append(container.find(curr_class, class_=&#34;ipc-inline-list__item&#34;).text)
            # return res
        elif curr_str == &#39;language&#39;:
            data_language = container.find(curr_class, attrs={&#39;class&#39;: &#39;ipc-metadata-list__item&#39;,
                                                              &#39;data-testid&#39;: &#39;title-details-languages&#39;})
            res.append(data_language.a.text)
            return res

    def getCrew(self, container, curr_class, curr_id):
        crew_ = container.find(curr_class, attrs={&#39;class&#39;: &#34;dataHeaderWithBorder&#34;, &#39;id&#39;: curr_id})
        if crew_ == None:
            return None
        else:
            contents = crew_.next_sibling.next_sibling
            all_cast = contents.find_all(&#39;tr&#39;)
            cr_list = []
            for i in all_cast:
                if i.find(&#39;a&#39;) == None:
                    continue
                else:
                    name_ = i.find(&#39;a&#39;).text.replace(&#39;\n&#39;, &#39;&#39;)
                    cr_list.append(name_)

            return cr_list

    def geCast(self, container, curr_class):
        cast_list = []
        for i in container:
            act_ = defaultdict(str)
            char_ = defaultdict(str)
            act_[&#39;actor&#39;] = i.find(curr_class, attrs={&#39;data-testid&#39;: &#39;title-cast-item__actor&#39;}).text
            char_[&#39;role&#39;] = i.find(curr_class, attrs={&#39;data-testid&#39;: &#39;cast-item-characters-link&#39;}).span.text
            cast_list.append((act_, char_))

        return cast_list

    def getResponse(self):

        response = requests.get(self.url)
        soup = BeautifulSoup(response.content, &#39;html.parser&#39;)
        data_div = soup.findAll(&#39;div&#39;, attrs={&#39;class&#39;: &#39;lister-item mode-advanced&#39;})
        count = 0

        for container in data_div:
            print(count)
            count += 1
            curr_info = defaultdict(str)
            raw_id = container.h3.a[&#39;href&#39;]
            curr_id = raw_id.split(&#39;/&#39;)[2]
            self.doc_id.append(curr_id)
            curr_info[&#39;docid&#39;] = curr_id
            curr_info[&#39;title&#39;] = container.h3.a.text
            curr_info[&#39;year&#39;] = container.h3.find(&#39;span&#39;, class_=&#39;lister-item-year text-muted unbold&#39;).text.replace(&#39;(&#39;,
                                                                                                                    &#39;&#39;).replace(
                &#39;)&#39;, &#39;&#39;)
            curr_info[&#39;runningtimes&#39;] = self.certainResponse(container, &#39;span&#39;, &#39;runtime&#39;)
            curr_info[&#39;genres&#39;] = self.certainResponse(container, &#39;span&#39;, &#39;genre&#39;)
            all_in_text_muted = container.find_all(&#39;p&#39;, class_=&#39;text-muted&#39;)
            curr_info[&#39;plot&#39;] = all_in_text_muted[1].text.replace(&#39;\n&#39;, &#39;&#39;) if len(all_in_text_muted) &gt; 1 else &#39;*****&#39;

            # Access to the detail page
            curr_ = self.url_baseline + &#39;/title/&#39; + curr_id + &#39;/&#39;
            req = requests.get(url=curr_, headers={&#39;User-Agent&#39;: &#39;Mozilla/5.0&#39;}).text
            curr_page = BeautifulSoup(req, &#39;html.parser&#39;)
            data_detail = curr_page.find(&#39;div&#39;, attrs={&#39;data-testid&#39;: &#39;title-details-section&#39;,
                                                       &#39;class&#39;: &#39;sc-f65f65be-0 fVkLRr&#39;})

            curr_info[&#39;countries&#39;] = self.certainResponse(data_detail, &#39;li&#39;, &#39;country&#39;)
            curr_info[&#39;languages&#39;] = self.certainResponse(data_detail, &#39;li&#39;, &#39;language&#39;)

            data_tech = curr_page.find(&#39;div&#39;, attrs={&#39;data-testid&#39;: &#39;title-techspecs-section&#39;,
                                                     &#39;class&#39;: &#39;sc-f65f65be-0 fVkLRr&#39;})
            curr_info[&#39;colorinfos&#39;] = self.certainResponse(data_tech, &#39;li&#39;, &#39;color&#39;)
            curr_info[&#39;soundmixes&#39;] = self.certainResponse(data_tech, &#39;li&#39;, &#39;soundmixes&#39;)

            data_cast = curr_page.findAll(&#39;div&#39;,
                                          attrs={&#39;data-testid&#39;: &#39;title-cast-item&#39;, &#39;class&#39;: &#39;sc-bfec09a1-5 kUzsHJ&#39;})
            curr_info[&#39;cast&#39;] = self.geCast(data_cast, &#39;a&#39;)

            data_type = curr_page.find(&#39;meta&#39;, attrs={&#39;property&#39;: {&#39;og:type&#39;}})
            curr_info[&#39;type&#39;] = data_type[&#39;content&#39;].split(&#39;.&#39;)[1]

            releaseInfo_ = self.url_baseline + &#39;/title/&#39; + curr_id + &#39;/releaseinfo/?ref_=tt_dt_rdat&#39;
            req_re = requests.get(url=releaseInfo_, headers={&#39;User-Agent&#39;: &#39;Mozilla/5.0&#39;}).text
            re_soup = BeautifulSoup(req_re, &#39;html.parser&#39;)

            curr_info[&#39;releasedates&#39;] = self.certainResponse(re_soup, &#39;div&#39;, &#39;releasedate&#39;)

            certificateInfo_ = self.url_baseline + &#39;/title/&#39; + curr_id + &#39;/parentalguide?ref_=tt_stry_pg&#39;
            req_cer = requests.get(url=certificateInfo_, headers={&#39;User-Agent&#39;: &#39;Mozilla/5.0&#39;}).text
            cer_soup = BeautifulSoup(req_cer, &#39;html.parser&#39;)

            curr_info[&#39;certificates&#39;] = self.certainResponse(cer_soup, &#39;tr&#39;, &#39;certificate&#39;)

            keywords_ = self.url_baseline + &#39;/title/&#39; + curr_id + &#39;/keywords?ref_=tt_stry_kw&#39;
            req_k = requests.get(url=keywords_, headers={&#39;User-Agent&#39;: &#39;Mozilla/5.0&#39;}).text
            k_soup = BeautifulSoup(req_k, &#39;html.parser&#39;)

            curr_info[&#39;keywords&#39;] = self.certainResponse(k_soup, &#39;td&#39;, &#39;keywords&#39;)

            cast_crew_ = self.url_baseline + &#39;/title/&#39; + curr_id + &#39;/fullcredits/?ref_=tt_cl_sm&#39;
            req_c = requests.get(url=cast_crew_, headers={&#39;User-Agent&#39;: &#39;Mozilla/5.0&#39;}).text
            c_soup = BeautifulSoup(req_c, &#39;html.parser&#39;)

            curr_info[&#39;directors&#39;] = self.getCrew(c_soup, &#39;h4&#39;, &#39;director&#39;)
            curr_info[&#39;composers&#39;] = self.getCrew(c_soup, &#39;h4&#39;, &#39;composer&#39;)
            curr_info[&#39;producers&#39;] = self.getCrew(c_soup, &#39;h4&#39;, &#39;producer&#39;)
            curr_info[&#39;writers&#39;] = self.getCrew(c_soup, &#39;h4&#39;, &#39;writer&#39;)
            curr_info[&#39;editors&#39;] = self.getCrew(c_soup, &#39;h4&#39;, &#39;editor&#39;)

            # print(curr_info)
            self.allInfo[curr_id] = curr_info

    def dict2XML(self):
        for id, values in self.allInfo.items():
            # Create the root element
            root = ET.Element(&#39;doc&#39;, {&#39;id&#39;: id})

            # Loop over each value in the inner defaultdict
            for key, value in values.items():
                # If the value is a list, loop over it and create a separate XML element for each item
                if isinstance(value, list):
                    sub = ET.SubElement(root, key)
                    if key == &#39;runningtimes&#39;:
                        for item in value:
                            ET.SubElement(sub, key[:-1], {&#39;country&#39;: &#34;default&#34;}).text = item
                    elif key == &#39;certificates&#39;:
                        for item in value:
                            ET.SubElement(sub, key[:-1], {&#39;country&#39;: item[0]}).text = item[1]
                    elif key == &#39;releasedates&#39;:
                        for item in value:
                            ET.SubElement(sub, key[:-1], {&#39;country&#39;: item[0]}).text = item[1]
                    elif key == &#39;cast&#39;:
                        for actor_dict, role_dict in value:
                            credit = ET.SubElement(sub, &#39;credit&#39;)
                            actor = ET.SubElement(credit, &#39;actor&#39;)
                            actor.text = actor_dict[&#39;actor&#39;]
                            role = ET.SubElement(credit, &#39;role&#39;)
                            role.text = role_dict[&#39;role&#39;]
                    elif key == &#39;countries&#39;:
                        for item in value:
                            ET.SubElement(sub, &#39;country&#39;).text = item
                    elif key == &#39;soundmixes&#39;:
                        for item in value:
                            ET.SubElement(sub, &#39;soundmix&#39;).text = item
                    else:
                        for item in value:
                            ET.SubElement(sub, key[:-1]).text = item
                else:
                    ET.SubElement(root, key).text = value

            # Create the XML tree and write it to a file with the id as its filename
            tree = ET.ElementTree(root)
            xmlstr = xml.dom.minidom.parseString(ET.tostring(root)).toprettyxml(indent=&#34;\t&#34;)

            with open(os.path.join(&#39;../Dataset/IMDB TestData&#39;, id + &#39;.xml&#39;), &#34;w&#34;, encoding=&#34;utf-8&#34;) as f:
                f.write(xmlstr)


if __name__ == &#39;__main__&#39;:
    parser = argparse.ArgumentParser()
    parser.add_argument(&#39;--url&#39;, type=str, help=&#39;URL&#39;)
    parser.add_argument(&#39;--url_baseline&#39;, type=str, help=&#39;detail URL&#39;)
    args = parser.parse_args()
    webScrap = webScraping(args.url, args.url_baseline)

    webScrap.getResponse()
    # webScrap.dict2XML()</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="Backend.webScrapin.webScraping"><code class="flex name class">
<span>class <span class="ident">webScraping</span></span>
<span>(</span><span>url, url_baseline)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class webScraping:
    def __init__(self, url, url_baseline):
        self.url = url
        self.url_baseline = url_baseline
        self.doc_id = []  # link to access the page of video details
        self.allInfo = defaultdict(lambda: defaultdict(str))  # {id1: {title: , year: , type: ,colorinfo: ,}, id2:{...}}

    def certainResponse(self, container, curr_class, curr_str):
        res = []
        if curr_str == &#39;runtime&#39;:
            temp = container.p.find(curr_class, class_=curr_str)
            if temp == None:
                return None
            else:
                res.append(container.p.find(curr_class, class_=curr_str).text.replace(&#39; min&#39;, &#39;&#39;))
                return res
        elif curr_str == &#39;certificate&#39;:
            data_cer = container.find(curr_class, attrs={&#39;class&#39;: &#39;ipl-zebra-list__item&#39;, &#39;id&#39;: &#39;certifications-list&#39;})
            cert_ = data_cer.find_all(&#39;li&#39;, attrs={&#39;class&#39;: &#39;ipl-inline-list__item&#39;})
            for i in cert_:
                raw_ = i.a.text.split(&#39;:&#39;)
                res.append((raw_[0], raw_[1]))
            return res
        elif curr_str == &#39;genre&#39;:
            res = []
            temp = container.p.find(curr_class, class_=curr_str)
            if temp == None:
                return None
            else:
                raw_ = container.find(curr_class, class_=curr_str).text.replace(&#39;\n&#39;, &#39;&#39;)
                return re.sub(&#39; +&#39;, &#39;&#39;, raw_).split(&#39;,&#39;)
        elif curr_str == &#39;color&#39;:
            res = []
            temp = container.find(curr_class,
                                  attrs={&#39;class&#39;: &#39;ipc-metadata-list__item&#39;, &#39;data-testid&#39;: &#39;title-techspec_color&#39;})
            if temp == None:
                return None
            else:
                res.append(temp.a.text)
                return res
        elif curr_str == &#39;soundmixes&#39;:
            temp = container.find(curr_class,
                                  attrs={&#39;class&#39;: &#39;ipc-metadata-list__item&#39;, &#39;data-testid&#39;: &#39;title-techspec_soundmix&#39;})
            if temp == None:
                return None
            else:
                res.append(temp.a.text)
                return res
        elif curr_str == &#39;keywords&#39;:
            k_list = []
            data_keyword_ = container.findAll(curr_class, attrs={&#39;class&#39;: &#39;soda sodavote&#39;})
            for i in data_keyword_:
                k_list.append(i[&#39;data-item-keyword&#39;])
            return k_list
        elif curr_str == &#39;country&#39;:
            data_country = container.find(curr_class, attrs={&#39;class&#39;: &#39;ipc-metadata-list__item&#39;,
                                                             &#39;data-testid&#39;: &#39;title-details-origin&#39;})
            res.append(data_country.a.text)
            return res
        elif curr_str == &#39;releasedate&#39;:
            date_list = []
            data_released = container.find(curr_class, attrs={&#39;data-testid&#39;: &#39;sub-section-releases&#39;,
                                                              &#39;class&#39;: &#39;sc-f65f65be-0 fVkLRr&#39;})
            date_ = data_released.find_all(&#39;li&#39;, attrs={&#39;data-testid&#39;: &#39;list-item&#39;})
            for i in date_:
                date_ = &#39;&#39;.join(i.label.text.split(&#39;,&#39;))
                date_list.append((i.a[&#39;aria-label&#39;], date_))
            return date_list
            # res.append(container.find(curr_class, class_=&#34;ipc-inline-list__item&#34;).text)
            # return res
        elif curr_str == &#39;language&#39;:
            data_language = container.find(curr_class, attrs={&#39;class&#39;: &#39;ipc-metadata-list__item&#39;,
                                                              &#39;data-testid&#39;: &#39;title-details-languages&#39;})
            res.append(data_language.a.text)
            return res

    def getCrew(self, container, curr_class, curr_id):
        crew_ = container.find(curr_class, attrs={&#39;class&#39;: &#34;dataHeaderWithBorder&#34;, &#39;id&#39;: curr_id})
        if crew_ == None:
            return None
        else:
            contents = crew_.next_sibling.next_sibling
            all_cast = contents.find_all(&#39;tr&#39;)
            cr_list = []
            for i in all_cast:
                if i.find(&#39;a&#39;) == None:
                    continue
                else:
                    name_ = i.find(&#39;a&#39;).text.replace(&#39;\n&#39;, &#39;&#39;)
                    cr_list.append(name_)

            return cr_list

    def geCast(self, container, curr_class):
        cast_list = []
        for i in container:
            act_ = defaultdict(str)
            char_ = defaultdict(str)
            act_[&#39;actor&#39;] = i.find(curr_class, attrs={&#39;data-testid&#39;: &#39;title-cast-item__actor&#39;}).text
            char_[&#39;role&#39;] = i.find(curr_class, attrs={&#39;data-testid&#39;: &#39;cast-item-characters-link&#39;}).span.text
            cast_list.append((act_, char_))

        return cast_list

    def getResponse(self):

        response = requests.get(self.url)
        soup = BeautifulSoup(response.content, &#39;html.parser&#39;)
        data_div = soup.findAll(&#39;div&#39;, attrs={&#39;class&#39;: &#39;lister-item mode-advanced&#39;})
        count = 0

        for container in data_div:
            print(count)
            count += 1
            curr_info = defaultdict(str)
            raw_id = container.h3.a[&#39;href&#39;]
            curr_id = raw_id.split(&#39;/&#39;)[2]
            self.doc_id.append(curr_id)
            curr_info[&#39;docid&#39;] = curr_id
            curr_info[&#39;title&#39;] = container.h3.a.text
            curr_info[&#39;year&#39;] = container.h3.find(&#39;span&#39;, class_=&#39;lister-item-year text-muted unbold&#39;).text.replace(&#39;(&#39;,
                                                                                                                    &#39;&#39;).replace(
                &#39;)&#39;, &#39;&#39;)
            curr_info[&#39;runningtimes&#39;] = self.certainResponse(container, &#39;span&#39;, &#39;runtime&#39;)
            curr_info[&#39;genres&#39;] = self.certainResponse(container, &#39;span&#39;, &#39;genre&#39;)
            all_in_text_muted = container.find_all(&#39;p&#39;, class_=&#39;text-muted&#39;)
            curr_info[&#39;plot&#39;] = all_in_text_muted[1].text.replace(&#39;\n&#39;, &#39;&#39;) if len(all_in_text_muted) &gt; 1 else &#39;*****&#39;

            # Access to the detail page
            curr_ = self.url_baseline + &#39;/title/&#39; + curr_id + &#39;/&#39;
            req = requests.get(url=curr_, headers={&#39;User-Agent&#39;: &#39;Mozilla/5.0&#39;}).text
            curr_page = BeautifulSoup(req, &#39;html.parser&#39;)
            data_detail = curr_page.find(&#39;div&#39;, attrs={&#39;data-testid&#39;: &#39;title-details-section&#39;,
                                                       &#39;class&#39;: &#39;sc-f65f65be-0 fVkLRr&#39;})

            curr_info[&#39;countries&#39;] = self.certainResponse(data_detail, &#39;li&#39;, &#39;country&#39;)
            curr_info[&#39;languages&#39;] = self.certainResponse(data_detail, &#39;li&#39;, &#39;language&#39;)

            data_tech = curr_page.find(&#39;div&#39;, attrs={&#39;data-testid&#39;: &#39;title-techspecs-section&#39;,
                                                     &#39;class&#39;: &#39;sc-f65f65be-0 fVkLRr&#39;})
            curr_info[&#39;colorinfos&#39;] = self.certainResponse(data_tech, &#39;li&#39;, &#39;color&#39;)
            curr_info[&#39;soundmixes&#39;] = self.certainResponse(data_tech, &#39;li&#39;, &#39;soundmixes&#39;)

            data_cast = curr_page.findAll(&#39;div&#39;,
                                          attrs={&#39;data-testid&#39;: &#39;title-cast-item&#39;, &#39;class&#39;: &#39;sc-bfec09a1-5 kUzsHJ&#39;})
            curr_info[&#39;cast&#39;] = self.geCast(data_cast, &#39;a&#39;)

            data_type = curr_page.find(&#39;meta&#39;, attrs={&#39;property&#39;: {&#39;og:type&#39;}})
            curr_info[&#39;type&#39;] = data_type[&#39;content&#39;].split(&#39;.&#39;)[1]

            releaseInfo_ = self.url_baseline + &#39;/title/&#39; + curr_id + &#39;/releaseinfo/?ref_=tt_dt_rdat&#39;
            req_re = requests.get(url=releaseInfo_, headers={&#39;User-Agent&#39;: &#39;Mozilla/5.0&#39;}).text
            re_soup = BeautifulSoup(req_re, &#39;html.parser&#39;)

            curr_info[&#39;releasedates&#39;] = self.certainResponse(re_soup, &#39;div&#39;, &#39;releasedate&#39;)

            certificateInfo_ = self.url_baseline + &#39;/title/&#39; + curr_id + &#39;/parentalguide?ref_=tt_stry_pg&#39;
            req_cer = requests.get(url=certificateInfo_, headers={&#39;User-Agent&#39;: &#39;Mozilla/5.0&#39;}).text
            cer_soup = BeautifulSoup(req_cer, &#39;html.parser&#39;)

            curr_info[&#39;certificates&#39;] = self.certainResponse(cer_soup, &#39;tr&#39;, &#39;certificate&#39;)

            keywords_ = self.url_baseline + &#39;/title/&#39; + curr_id + &#39;/keywords?ref_=tt_stry_kw&#39;
            req_k = requests.get(url=keywords_, headers={&#39;User-Agent&#39;: &#39;Mozilla/5.0&#39;}).text
            k_soup = BeautifulSoup(req_k, &#39;html.parser&#39;)

            curr_info[&#39;keywords&#39;] = self.certainResponse(k_soup, &#39;td&#39;, &#39;keywords&#39;)

            cast_crew_ = self.url_baseline + &#39;/title/&#39; + curr_id + &#39;/fullcredits/?ref_=tt_cl_sm&#39;
            req_c = requests.get(url=cast_crew_, headers={&#39;User-Agent&#39;: &#39;Mozilla/5.0&#39;}).text
            c_soup = BeautifulSoup(req_c, &#39;html.parser&#39;)

            curr_info[&#39;directors&#39;] = self.getCrew(c_soup, &#39;h4&#39;, &#39;director&#39;)
            curr_info[&#39;composers&#39;] = self.getCrew(c_soup, &#39;h4&#39;, &#39;composer&#39;)
            curr_info[&#39;producers&#39;] = self.getCrew(c_soup, &#39;h4&#39;, &#39;producer&#39;)
            curr_info[&#39;writers&#39;] = self.getCrew(c_soup, &#39;h4&#39;, &#39;writer&#39;)
            curr_info[&#39;editors&#39;] = self.getCrew(c_soup, &#39;h4&#39;, &#39;editor&#39;)

            # print(curr_info)
            self.allInfo[curr_id] = curr_info

    def dict2XML(self):
        for id, values in self.allInfo.items():
            # Create the root element
            root = ET.Element(&#39;doc&#39;, {&#39;id&#39;: id})

            # Loop over each value in the inner defaultdict
            for key, value in values.items():
                # If the value is a list, loop over it and create a separate XML element for each item
                if isinstance(value, list):
                    sub = ET.SubElement(root, key)
                    if key == &#39;runningtimes&#39;:
                        for item in value:
                            ET.SubElement(sub, key[:-1], {&#39;country&#39;: &#34;default&#34;}).text = item
                    elif key == &#39;certificates&#39;:
                        for item in value:
                            ET.SubElement(sub, key[:-1], {&#39;country&#39;: item[0]}).text = item[1]
                    elif key == &#39;releasedates&#39;:
                        for item in value:
                            ET.SubElement(sub, key[:-1], {&#39;country&#39;: item[0]}).text = item[1]
                    elif key == &#39;cast&#39;:
                        for actor_dict, role_dict in value:
                            credit = ET.SubElement(sub, &#39;credit&#39;)
                            actor = ET.SubElement(credit, &#39;actor&#39;)
                            actor.text = actor_dict[&#39;actor&#39;]
                            role = ET.SubElement(credit, &#39;role&#39;)
                            role.text = role_dict[&#39;role&#39;]
                    elif key == &#39;countries&#39;:
                        for item in value:
                            ET.SubElement(sub, &#39;country&#39;).text = item
                    elif key == &#39;soundmixes&#39;:
                        for item in value:
                            ET.SubElement(sub, &#39;soundmix&#39;).text = item
                    else:
                        for item in value:
                            ET.SubElement(sub, key[:-1]).text = item
                else:
                    ET.SubElement(root, key).text = value

            # Create the XML tree and write it to a file with the id as its filename
            tree = ET.ElementTree(root)
            xmlstr = xml.dom.minidom.parseString(ET.tostring(root)).toprettyxml(indent=&#34;\t&#34;)

            with open(os.path.join(&#39;../Dataset/IMDB TestData&#39;, id + &#39;.xml&#39;), &#34;w&#34;, encoding=&#34;utf-8&#34;) as f:
                f.write(xmlstr)</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="Backend.webScrapin.webScraping.certainResponse"><code class="name flex">
<span>def <span class="ident">certainResponse</span></span>(<span>self, container, curr_class, curr_str)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def certainResponse(self, container, curr_class, curr_str):
    res = []
    if curr_str == &#39;runtime&#39;:
        temp = container.p.find(curr_class, class_=curr_str)
        if temp == None:
            return None
        else:
            res.append(container.p.find(curr_class, class_=curr_str).text.replace(&#39; min&#39;, &#39;&#39;))
            return res
    elif curr_str == &#39;certificate&#39;:
        data_cer = container.find(curr_class, attrs={&#39;class&#39;: &#39;ipl-zebra-list__item&#39;, &#39;id&#39;: &#39;certifications-list&#39;})
        cert_ = data_cer.find_all(&#39;li&#39;, attrs={&#39;class&#39;: &#39;ipl-inline-list__item&#39;})
        for i in cert_:
            raw_ = i.a.text.split(&#39;:&#39;)
            res.append((raw_[0], raw_[1]))
        return res
    elif curr_str == &#39;genre&#39;:
        res = []
        temp = container.p.find(curr_class, class_=curr_str)
        if temp == None:
            return None
        else:
            raw_ = container.find(curr_class, class_=curr_str).text.replace(&#39;\n&#39;, &#39;&#39;)
            return re.sub(&#39; +&#39;, &#39;&#39;, raw_).split(&#39;,&#39;)
    elif curr_str == &#39;color&#39;:
        res = []
        temp = container.find(curr_class,
                              attrs={&#39;class&#39;: &#39;ipc-metadata-list__item&#39;, &#39;data-testid&#39;: &#39;title-techspec_color&#39;})
        if temp == None:
            return None
        else:
            res.append(temp.a.text)
            return res
    elif curr_str == &#39;soundmixes&#39;:
        temp = container.find(curr_class,
                              attrs={&#39;class&#39;: &#39;ipc-metadata-list__item&#39;, &#39;data-testid&#39;: &#39;title-techspec_soundmix&#39;})
        if temp == None:
            return None
        else:
            res.append(temp.a.text)
            return res
    elif curr_str == &#39;keywords&#39;:
        k_list = []
        data_keyword_ = container.findAll(curr_class, attrs={&#39;class&#39;: &#39;soda sodavote&#39;})
        for i in data_keyword_:
            k_list.append(i[&#39;data-item-keyword&#39;])
        return k_list
    elif curr_str == &#39;country&#39;:
        data_country = container.find(curr_class, attrs={&#39;class&#39;: &#39;ipc-metadata-list__item&#39;,
                                                         &#39;data-testid&#39;: &#39;title-details-origin&#39;})
        res.append(data_country.a.text)
        return res
    elif curr_str == &#39;releasedate&#39;:
        date_list = []
        data_released = container.find(curr_class, attrs={&#39;data-testid&#39;: &#39;sub-section-releases&#39;,
                                                          &#39;class&#39;: &#39;sc-f65f65be-0 fVkLRr&#39;})
        date_ = data_released.find_all(&#39;li&#39;, attrs={&#39;data-testid&#39;: &#39;list-item&#39;})
        for i in date_:
            date_ = &#39;&#39;.join(i.label.text.split(&#39;,&#39;))
            date_list.append((i.a[&#39;aria-label&#39;], date_))
        return date_list
        # res.append(container.find(curr_class, class_=&#34;ipc-inline-list__item&#34;).text)
        # return res
    elif curr_str == &#39;language&#39;:
        data_language = container.find(curr_class, attrs={&#39;class&#39;: &#39;ipc-metadata-list__item&#39;,
                                                          &#39;data-testid&#39;: &#39;title-details-languages&#39;})
        res.append(data_language.a.text)
        return res</code></pre>
</details>
</dd>
<dt id="Backend.webScrapin.webScraping.dict2XML"><code class="name flex">
<span>def <span class="ident">dict2XML</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def dict2XML(self):
    for id, values in self.allInfo.items():
        # Create the root element
        root = ET.Element(&#39;doc&#39;, {&#39;id&#39;: id})

        # Loop over each value in the inner defaultdict
        for key, value in values.items():
            # If the value is a list, loop over it and create a separate XML element for each item
            if isinstance(value, list):
                sub = ET.SubElement(root, key)
                if key == &#39;runningtimes&#39;:
                    for item in value:
                        ET.SubElement(sub, key[:-1], {&#39;country&#39;: &#34;default&#34;}).text = item
                elif key == &#39;certificates&#39;:
                    for item in value:
                        ET.SubElement(sub, key[:-1], {&#39;country&#39;: item[0]}).text = item[1]
                elif key == &#39;releasedates&#39;:
                    for item in value:
                        ET.SubElement(sub, key[:-1], {&#39;country&#39;: item[0]}).text = item[1]
                elif key == &#39;cast&#39;:
                    for actor_dict, role_dict in value:
                        credit = ET.SubElement(sub, &#39;credit&#39;)
                        actor = ET.SubElement(credit, &#39;actor&#39;)
                        actor.text = actor_dict[&#39;actor&#39;]
                        role = ET.SubElement(credit, &#39;role&#39;)
                        role.text = role_dict[&#39;role&#39;]
                elif key == &#39;countries&#39;:
                    for item in value:
                        ET.SubElement(sub, &#39;country&#39;).text = item
                elif key == &#39;soundmixes&#39;:
                    for item in value:
                        ET.SubElement(sub, &#39;soundmix&#39;).text = item
                else:
                    for item in value:
                        ET.SubElement(sub, key[:-1]).text = item
            else:
                ET.SubElement(root, key).text = value

        # Create the XML tree and write it to a file with the id as its filename
        tree = ET.ElementTree(root)
        xmlstr = xml.dom.minidom.parseString(ET.tostring(root)).toprettyxml(indent=&#34;\t&#34;)

        with open(os.path.join(&#39;../Dataset/IMDB TestData&#39;, id + &#39;.xml&#39;), &#34;w&#34;, encoding=&#34;utf-8&#34;) as f:
            f.write(xmlstr)</code></pre>
</details>
</dd>
<dt id="Backend.webScrapin.webScraping.geCast"><code class="name flex">
<span>def <span class="ident">geCast</span></span>(<span>self, container, curr_class)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def geCast(self, container, curr_class):
    cast_list = []
    for i in container:
        act_ = defaultdict(str)
        char_ = defaultdict(str)
        act_[&#39;actor&#39;] = i.find(curr_class, attrs={&#39;data-testid&#39;: &#39;title-cast-item__actor&#39;}).text
        char_[&#39;role&#39;] = i.find(curr_class, attrs={&#39;data-testid&#39;: &#39;cast-item-characters-link&#39;}).span.text
        cast_list.append((act_, char_))

    return cast_list</code></pre>
</details>
</dd>
<dt id="Backend.webScrapin.webScraping.getCrew"><code class="name flex">
<span>def <span class="ident">getCrew</span></span>(<span>self, container, curr_class, curr_id)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def getCrew(self, container, curr_class, curr_id):
    crew_ = container.find(curr_class, attrs={&#39;class&#39;: &#34;dataHeaderWithBorder&#34;, &#39;id&#39;: curr_id})
    if crew_ == None:
        return None
    else:
        contents = crew_.next_sibling.next_sibling
        all_cast = contents.find_all(&#39;tr&#39;)
        cr_list = []
        for i in all_cast:
            if i.find(&#39;a&#39;) == None:
                continue
            else:
                name_ = i.find(&#39;a&#39;).text.replace(&#39;\n&#39;, &#39;&#39;)
                cr_list.append(name_)

        return cr_list</code></pre>
</details>
</dd>
<dt id="Backend.webScrapin.webScraping.getResponse"><code class="name flex">
<span>def <span class="ident">getResponse</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def getResponse(self):

    response = requests.get(self.url)
    soup = BeautifulSoup(response.content, &#39;html.parser&#39;)
    data_div = soup.findAll(&#39;div&#39;, attrs={&#39;class&#39;: &#39;lister-item mode-advanced&#39;})
    count = 0

    for container in data_div:
        print(count)
        count += 1
        curr_info = defaultdict(str)
        raw_id = container.h3.a[&#39;href&#39;]
        curr_id = raw_id.split(&#39;/&#39;)[2]
        self.doc_id.append(curr_id)
        curr_info[&#39;docid&#39;] = curr_id
        curr_info[&#39;title&#39;] = container.h3.a.text
        curr_info[&#39;year&#39;] = container.h3.find(&#39;span&#39;, class_=&#39;lister-item-year text-muted unbold&#39;).text.replace(&#39;(&#39;,
                                                                                                                &#39;&#39;).replace(
            &#39;)&#39;, &#39;&#39;)
        curr_info[&#39;runningtimes&#39;] = self.certainResponse(container, &#39;span&#39;, &#39;runtime&#39;)
        curr_info[&#39;genres&#39;] = self.certainResponse(container, &#39;span&#39;, &#39;genre&#39;)
        all_in_text_muted = container.find_all(&#39;p&#39;, class_=&#39;text-muted&#39;)
        curr_info[&#39;plot&#39;] = all_in_text_muted[1].text.replace(&#39;\n&#39;, &#39;&#39;) if len(all_in_text_muted) &gt; 1 else &#39;*****&#39;

        # Access to the detail page
        curr_ = self.url_baseline + &#39;/title/&#39; + curr_id + &#39;/&#39;
        req = requests.get(url=curr_, headers={&#39;User-Agent&#39;: &#39;Mozilla/5.0&#39;}).text
        curr_page = BeautifulSoup(req, &#39;html.parser&#39;)
        data_detail = curr_page.find(&#39;div&#39;, attrs={&#39;data-testid&#39;: &#39;title-details-section&#39;,
                                                   &#39;class&#39;: &#39;sc-f65f65be-0 fVkLRr&#39;})

        curr_info[&#39;countries&#39;] = self.certainResponse(data_detail, &#39;li&#39;, &#39;country&#39;)
        curr_info[&#39;languages&#39;] = self.certainResponse(data_detail, &#39;li&#39;, &#39;language&#39;)

        data_tech = curr_page.find(&#39;div&#39;, attrs={&#39;data-testid&#39;: &#39;title-techspecs-section&#39;,
                                                 &#39;class&#39;: &#39;sc-f65f65be-0 fVkLRr&#39;})
        curr_info[&#39;colorinfos&#39;] = self.certainResponse(data_tech, &#39;li&#39;, &#39;color&#39;)
        curr_info[&#39;soundmixes&#39;] = self.certainResponse(data_tech, &#39;li&#39;, &#39;soundmixes&#39;)

        data_cast = curr_page.findAll(&#39;div&#39;,
                                      attrs={&#39;data-testid&#39;: &#39;title-cast-item&#39;, &#39;class&#39;: &#39;sc-bfec09a1-5 kUzsHJ&#39;})
        curr_info[&#39;cast&#39;] = self.geCast(data_cast, &#39;a&#39;)

        data_type = curr_page.find(&#39;meta&#39;, attrs={&#39;property&#39;: {&#39;og:type&#39;}})
        curr_info[&#39;type&#39;] = data_type[&#39;content&#39;].split(&#39;.&#39;)[1]

        releaseInfo_ = self.url_baseline + &#39;/title/&#39; + curr_id + &#39;/releaseinfo/?ref_=tt_dt_rdat&#39;
        req_re = requests.get(url=releaseInfo_, headers={&#39;User-Agent&#39;: &#39;Mozilla/5.0&#39;}).text
        re_soup = BeautifulSoup(req_re, &#39;html.parser&#39;)

        curr_info[&#39;releasedates&#39;] = self.certainResponse(re_soup, &#39;div&#39;, &#39;releasedate&#39;)

        certificateInfo_ = self.url_baseline + &#39;/title/&#39; + curr_id + &#39;/parentalguide?ref_=tt_stry_pg&#39;
        req_cer = requests.get(url=certificateInfo_, headers={&#39;User-Agent&#39;: &#39;Mozilla/5.0&#39;}).text
        cer_soup = BeautifulSoup(req_cer, &#39;html.parser&#39;)

        curr_info[&#39;certificates&#39;] = self.certainResponse(cer_soup, &#39;tr&#39;, &#39;certificate&#39;)

        keywords_ = self.url_baseline + &#39;/title/&#39; + curr_id + &#39;/keywords?ref_=tt_stry_kw&#39;
        req_k = requests.get(url=keywords_, headers={&#39;User-Agent&#39;: &#39;Mozilla/5.0&#39;}).text
        k_soup = BeautifulSoup(req_k, &#39;html.parser&#39;)

        curr_info[&#39;keywords&#39;] = self.certainResponse(k_soup, &#39;td&#39;, &#39;keywords&#39;)

        cast_crew_ = self.url_baseline + &#39;/title/&#39; + curr_id + &#39;/fullcredits/?ref_=tt_cl_sm&#39;
        req_c = requests.get(url=cast_crew_, headers={&#39;User-Agent&#39;: &#39;Mozilla/5.0&#39;}).text
        c_soup = BeautifulSoup(req_c, &#39;html.parser&#39;)

        curr_info[&#39;directors&#39;] = self.getCrew(c_soup, &#39;h4&#39;, &#39;director&#39;)
        curr_info[&#39;composers&#39;] = self.getCrew(c_soup, &#39;h4&#39;, &#39;composer&#39;)
        curr_info[&#39;producers&#39;] = self.getCrew(c_soup, &#39;h4&#39;, &#39;producer&#39;)
        curr_info[&#39;writers&#39;] = self.getCrew(c_soup, &#39;h4&#39;, &#39;writer&#39;)
        curr_info[&#39;editors&#39;] = self.getCrew(c_soup, &#39;h4&#39;, &#39;editor&#39;)

        # print(curr_info)
        self.allInfo[curr_id] = curr_info</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="Backend" href="index.html">Backend</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="Backend.webScrapin.webScraping" href="#Backend.webScrapin.webScraping">webScraping</a></code></h4>
<ul class="">
<li><code><a title="Backend.webScrapin.webScraping.certainResponse" href="#Backend.webScrapin.webScraping.certainResponse">certainResponse</a></code></li>
<li><code><a title="Backend.webScrapin.webScraping.dict2XML" href="#Backend.webScrapin.webScraping.dict2XML">dict2XML</a></code></li>
<li><code><a title="Backend.webScrapin.webScraping.geCast" href="#Backend.webScrapin.webScraping.geCast">geCast</a></code></li>
<li><code><a title="Backend.webScrapin.webScraping.getCrew" href="#Backend.webScrapin.webScraping.getCrew">getCrew</a></code></li>
<li><code><a title="Backend.webScrapin.webScraping.getResponse" href="#Backend.webScrapin.webScraping.getResponse">getResponse</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>